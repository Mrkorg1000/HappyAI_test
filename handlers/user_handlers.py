import asyncio
from io import BytesIO
import json
from typing import Dict, List, Optional
from aiogram import Router, types
from aiogram.enums import ParseMode
from aiogram.filters import CommandStart, StateFilter
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram import F, types
from sqlalchemy.ext.asyncio import AsyncSession

from amplitude_dep import async_amplitude_track
from config import settings

from form import Form
from pg_db.database import async_session_maker
from services.assistant_client_service import client
from services.audio_to_text_service import audio_to_text
from services.assistant_client_service import get_single_response
from services.photo_service import analyze_mood
from services.values_service import save_user_values, user_has_values
from services.func_calling_service import VALUES_SYSTEM_PROMPT, tools
from services.text_to_audio_service import text_to_audio


user_router = Router()


@user_router.message(CommandStart())
async def start(message: types.Message) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–º–∞–Ω–¥—É /start –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - message (types.Message): –û–±—ä–µ–∫—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    await async_amplitude_track(
        user_id=message.from_user.id,
        event_type="start_command"
    )
    await message.answer(
        f"–ü—Ä–∏–≤–µ—Ç! <b>{message.from_user.first_name}</b>\n"
        "–Ø –æ—Ç–≤–µ—á—É –Ω–∞ —Ç–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã! –¢–æ–ª—å–∫–æ –≥–æ–ª–æ—Å–æ–≤—ã–µ!!!",
        parse_mode=ParseMode.HTML
    )
    await message.answer("–ê –µ—â–µ! –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Ñ–æ—Ç–æ —Å –ª–∏—Ü–æ–º, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é —Ç–≤–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ.")


@user_router.message(lambda message: message.text)
async def process_text(message: types.Message) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —É–≤–µ–¥–æ–º–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è,
    —á—Ç–æ –±–æ—Ç –æ—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –≥–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - message (types.Message): –û–±—ä–µ–∫—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    await async_amplitude_track(
        user_id=message.from_user.id,
        event_type="text_message_rejected"
    )
    await message.answer(("–û–±—â–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ—Å–æ–º!\n–ò–∑–≤–∏–Ω–∏, –ë—Ä–∞—Ç, —Ç–∞–∫–æ–µ –ó–∞–¥–∞–Ω–∏–µ üòî"))


@user_router.message(lambda message: message.voice,
                     ~StateFilter(Form.collecting_values),
)
async def process_voice_question(message: types.Message, state: FSMContext) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏—Ö –≤ —Ç–µ–∫—Å—Ç,
    –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –≤–∏–¥–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - message (types.Message): –û–±—ä–µ–∫—Ç –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    await async_amplitude_track(
        user_id=message.from_user.id,
        event_type="voice_message_received"
    )
    
    voice: types.Voice = message.voice
    file_id: str = voice.file_id

    await message.answer("–°–µ–∫—É–Ω–¥–æ—á–∫—É, —Å–µ–π—á–∞—Å –æ—Ç–≤–µ—á—É")

    # –°–∫–∞—á–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    file: types.File = await message.bot.get_file(file_id)
    downloaded_file: BytesIO = await message.bot.download_file(file.file_path)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç
    question_text: Optional[str] = await audio_to_text(downloaded_file)

    if question_text is None:
        await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="voice_recognition_failed"
            )
        await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
        return

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    response_text, thread_id = await get_single_response(question_text)

    if response_text is None:
        await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="assistant_response_failed"
            )
        await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.")
        return
    
    
    await state.update_data(thread_id=thread_id)


    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –≤ –∞—É–¥–∏–æ
    audio_response: Optional[BytesIO] = await text_to_audio(response_text, api_key=settings.OPENAI_API_KEY)

    if audio_response:
        audio_response.seek(0)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ
        voice_file = types.BufferedInputFile(
            audio_response.getvalue(), filename="response.ogg"
        )
        await message.answer_voice(voice_file)
        await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="voice_response_sent"
            )
    else:
        await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="audio_generation_failed"
            )
        await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ.")

    # await asyncio.sleep(3)
    
    telegram_id = message.from_user.id
    has_values = await user_has_values(telegram_id)
    if not has_values:
        await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="values_collection_started"
            )
        user_name = message.from_user.first_name
        values_question = f"{user_name}, –æ—Ç–≤–µ—Ç—å –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –∫–∞–∫–∏–µ —Ç–≤–æ–∏ –∂–∏–∑–Ω–µ–Ω–Ω—ã–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏. –ú–æ–∂–µ—à—å –Ω–∞–∑–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ."
    
        audio_response: Optional[BytesIO] = await text_to_audio(values_question, api_key=settings.OPENAI_API_KEY)

        if audio_response:
            audio_response.seek(0)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ
            voice_file = types.BufferedInputFile(
                audio_response.getvalue(), filename="response.ogg"
            )
            await message.answer_voice(voice_file)
            
            await state.set_state(Form.collecting_values)
            await state.update_data(
                conversation_history=[],
                attempt_count=0
            )
            
        else:
            await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ.")
        
        
@user_router.message(lambda message: message.voice, StateFilter(Form.collecting_values))
async def process_values(
    message: types.Message,
    state: FSMContext,
) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –æ –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç—è—Ö.
    """
    await async_amplitude_track(
        user_id=message.from_user.id,
        event_type="values_processing_started"
    )
    
    voice: types.Voice = message.voice
    file_id: str = voice.file_id

    await message.answer("–°–µ–∫—É–Ω–¥–æ—á–∫—É, —Å–µ–π—á–∞—Å –æ–±—Ä–∞–±–æ—Ç–∞—é —Ç–≤–æ–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏")

    # –°–∫–∞—á–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    file: types.File = await message.bot.get_file(file_id)
    downloaded_file: BytesIO = await message.bot.download_file(file.file_path)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç
    values_text: Optional[str] = await audio_to_text(downloaded_file)

    if values_text is None:
        await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="values_recognition_failed"
            )
        await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
        return
    
    state_data = await state.get_data()
    conversation_history: List[Dict[str, str]] = state_data.get("conversation_history", [])
    attempt_count: int = state_data.get("attempt_count", 0)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
    conversation_history.append({"role": "user", "content": values_text})
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è API
    messages_for_api = [{"role": "system", "content": VALUES_SYSTEM_PROMPT}] + conversation_history
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=messages_for_api,
            tools=tools,
            tool_choice="auto",
            temperature=0.5,
        )
        response_message = response.choices[0].message
        
        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                if tool_call.function.name == "save_user_values":
                    values = json.loads(tool_call.function.arguments)["values"]
                    
                    async with async_session_maker() as session:
                        await save_user_values(session, message.from_user.id, values)
                    await async_amplitude_track(
                        user_id=message.from_user.id,
                        event_type="values_saved",
                        event_props={"values_count": len(values)}
                    )
                    await message.answer("–ì–æ—Ç–æ–≤–æ, –í–∞—à–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã")
                    await state.clear()
                    return
                
        followup_question = response_message.content
        conversation_history.append(response_message.model_dump())
        
        if attempt_count >= 2:
            await message.answer("–î–∞–≤–∞–π—Ç–µ –ø—Ä–µ—Ä–≤—ë–º—Å—è. –í—ã –º–æ–∂–µ—Ç–µ –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ —ç—Ç–æ–º—É –ø–æ–∑–∂–µ.")
            await state.clear()
            return
        
        audio = await text_to_audio(followup_question, settings.OPENAI_API_KEY)
        if audio:
            await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="followup_question_sent"
            )
            await message.answer_voice(
                types.BufferedInputFile(audio.getvalue(), "followup.ogg")
            )

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ (—É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫)
        await state.update_data(
            conversation_history=conversation_history,
            attempt_count=attempt_count + 1
        )
        
    except Exception as e:
        await async_amplitude_track(
            user_id=message.from_user.id,
            event_type="values_processing_error",
            event_props={"error": str(e)[:100]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—à–∏–±–∫–∏
        )
        await message.answer("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        await state.clear()


@user_router.message(lambda message: message.photo is not None)
async def handle_photo(message: types.Message):
    """–•—ç–Ω–¥–ª–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ (—Å–∞–º–æ–µ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
        photo = message.photo[-1]
        file = await message.bot.get_file(photo.file_id)
        
        await async_amplitude_track(
            user_id=message.from_user.id,
            event_type="photo_received"
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Ñ–∞–π–ª–∞ –≤ Telegram
        file_url = f"https://api.telegram.org/file/bot{settings.BOT_TOKEN}/{file.file_path}"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é OpenAI
        mood_analysis = await analyze_mood(file_url)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        if "–õ–ò–¶–ê –ù–ï–¢" in mood_analysis:
            await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="no_face_detected"
            )
            await message.answer("üòï –ù–µ –≤–∏–∂—É –ª–∏—Ü–∞ –Ω–∞ —Ñ–æ—Ç–æ. –ü–æ–ø—Ä–æ–±—É–π —Å–¥–µ–ª–∞—Ç—å —Å–µ–ª—Ñ–∏!")
        elif "–û—à–∏–±–∫–∞" in mood_analysis:
            await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="mood_analysis_failed"
            )
            await message.answer("‚ö†Ô∏è –ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")
        else:
            await async_amplitude_track(
                user_id=message.from_user.id,
                event_type="mood_analyzed",
                event_props={"mood_result": mood_analysis}
            )
            response_text = f"–Ø –æ–ø—Ä–µ–¥–µ–ª–∏–ª —Ç–≤–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ:\n\n{mood_analysis}"
            audio_response: Optional[BytesIO] = await text_to_audio(response_text, api_key=settings.OPENAI_API_KEY)
            if audio_response:
                audio_response.seek(0)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ
                voice_file = types.BufferedInputFile(
                audio_response.getvalue(), filename="response.ogg"
                )
                await message.answer_voice(voice_file)
            else:
                await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ.")
                  
    except Exception as e:
        await async_amplitude_track(
            user_id=message.from_user.id,
            event_type="photo_processing_error",
            event_props={"error": str(e)}
        )
        await message.answer("üö´ –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–æ—Ç–æ.")
        
    
    
    
    
    
    
    
    
    